{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np0plMPXRvoq"
      },
      "source": [
        "# Construa Sua Primeira Equipe de Agentes Inteligentes: Um Bot de Clima Progressivo com ADK\n",
        "\n",
        "Este tutorial é uma extensão do [exemplo de Início Rápido](https://google.github.io/adk-docs/get-started/quickstart/) para o [Kit de Desenvolvimento de Agente (ADK)](https://google.github.io/adk-docs/get-started/). Agora, você está pronto para mergulhar mais fundo e construir um **sistema multiagente** mais sofisticado.\n",
        "\n",
        "Embarcaremos na construção de uma **equipe de agentes Bot de Clima**, adicionando progressivamente recursos avançados a uma base simples. Começando com um único agente que pode consultar o clima, adicionaremos incrementalmente capacidades como:\n",
        "\n",
        "*   Aproveitar diferentes modelos de IA (Gemini, GPT, Claude).\n",
        "*   Projetar subagentes especializados para tarefas distintas (como saudações e despedidas).\n",
        "*   Habilitar a delegação inteligente entre agentes.\n",
        "*   Dar memória aos agentes usando o estado de sessão persistente.\n",
        "*   Implementar barreiras de segurança cruciais usando callbacks.\n",
        "\n",
        "**Por que uma Equipe de Bot de Clima?**\n",
        "\n",
        "Este caso de uso, embora aparentemente simples, fornece uma base prática e compreensível para explorar conceitos centrais do ADK essenciais para a construção de aplicações agênticas complexas do mundo real. Você aprenderá como estruturar interações, gerenciar estados, garantir a segurança e orquestrar múltiplos \"cérebros\" de IA trabalhando em conjunto.\n",
        "\n",
        "**O que é o ADK mesmo?**\n",
        "\n",
        "Como lembrete, o ADK é um framework Python projetado para simplificar o desenvolvimento de aplicações impulsionadas por Modelos de Linguagem Grandes (LLMs). Ele oferece blocos de construção robustos para criar agentes que podem raciocinar, planejar, utilizar ferramentas, interagir dinamicamente com os usuários e colaborar de forma eficaz dentro de uma equipe.\n",
        "\n",
        "**Neste tutorial avançado, você irá dominar:**\n",
        "\n",
        "*   ✅ **Definição e Uso de Ferramentas:** Criar funções Python (`tools`) que concedem aos agentes habilidades específicas (como buscar dados) e instruir os agentes sobre como usá-las de forma eficaz.\n",
        "*   ✅ **Flexibilidade Multi-LLM:** Configurar agentes para utilizar vários LLMs líderes (Gemini, GPT-4o, Claude Sonnet) via integração com o LiteLLM, permitindo que você escolha o melhor modelo para cada tarefa.\n",
        "*   ✅ **Delegação e Colaboração de Agentes:** Projetar subagentes especializados e habilitar o roteamento automático (`auto flow`) de solicitações do usuário para o agente mais apropriado dentro de uma equipe.\n",
        "*   ✅ **Estado de Sessão para Memória:** Utilizar `Session State` e `ToolContext` para permitir que os agentes se lembrem de informações entre os turnos da conversa, levando a interações mais contextuais.\n",
        "*   ✅ **Barreiras de Segurança com Callbacks:** Implementar `before_model_callback` e `before_tool_callback` para inspecionar, modificar ou bloquear solicitações/uso de ferramentas com base em regras predefinidas, aumentando a segurança e o controle da aplicação.\n",
        "\n",
        "**Expectativa do Estado Final:**\n",
        "\n",
        "Ao concluir este tutorial, você terá construído um sistema funcional multiagente de Bot de Clima. Este sistema não apenas fornecerá informações sobre o tempo, mas também lidará com as gentilezas da conversação, lembrará a última cidade verificada e operará dentro de limites de segurança definidos, tudo orquestrado usando o ADK.\n",
        "\n",
        "**Pré-requisitos:**\n",
        "\n",
        "*   ✅ **Sólido conhecimento de programação em Python.**\n",
        "*   ✅ **Familiaridade com Modelos de Linguagem Grandes (LLMs), APIs e o conceito de agentes.**\n",
        "*   ❗ **Crucialmente: Conclusão do(s) tutorial(s) de Início Rápido do ADK ou conhecimento fundamental equivalente dos conceitos básicos do ADK (Agent, Runner, SessionService, uso básico de Tool).** Este tutorial se baseia diretamente nesses conceitos.\n",
        "*   ✅ **Chaves de API** para os LLMs que você pretende usar (ex: Google AI Studio para Gemini, Plataforma OpenAI, Console Anthropic).\n",
        "\n",
        "---\n",
        "**Nota sobre o Ambiente de Execução:**\n",
        "\n",
        "Este tutorial está estruturado para ambientes de notebook interativos como Google Colab, Colab Enterprise ou notebooks Jupyter. Por favor, tenha em mente o seguinte:\n",
        "\n",
        "*   **Executando Código Assíncrono:** Ambientes de notebook lidam com código assíncrono de maneira diferente. Você verá exemplos usando `await` (adequado quando um loop de eventos já está em execução, comum em notebooks) ou `asyncio.run()` (frequentemente necessário ao rodar como um script `.py` autônomo ou em configurações específicas de notebook). Os blocos de código fornecem orientação para ambos os cenários.\n",
        "*   **Configuração Manual de Runner/Session:** Os passos envolvem a criação explícita de instâncias de `Runner` e `SessionService`. Essa abordagem é mostrada porque lhe dá controle refinado sobre o ciclo de vida de execução do agente, gerenciamento de sessão e persistência de estado.\n",
        "\n",
        "**Alternativa: Usando as Ferramentas Integradas do ADK (UI Web / CLI / Servidor de API)**\n",
        "\n",
        "Se você prefere uma configuração que lida com o runner e o gerenciamento de sessão automaticamente usando as ferramentas padrão do ADK, pode encontrar o código equivalente estruturado para esse propósito [aqui](https://github.com/google/adk-docs/tree/main/examples/python/tutorial/agent_team/adk-tutorial). Essa versão é projetada para ser executada diretamente com comandos como `adk web` (para uma UI da web), `adk run` (para interação via CLI) ou `adk api_server` (para expor uma API). Por favor, siga as instruções do `README.md` fornecidas nesse recurso alternativo.\n",
        "\n",
        "---\n",
        "**Pronto para construir sua equipe de agentes? Vamos mergulhar de cabeça!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARCoeUZCRNGi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff55a486-898e-4510-98bc-635ccb36e6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalação concluída.\n"
          ]
        }
      ],
      "source": [
        "# @title Passo 0: Instalação e configuração\n",
        "# Instale o ADK e o LiteLLM para suporte a vários modelos\n",
        "\n",
        "!pip install google-adk -q\n",
        "!pip install litellm -q\n",
        "\n",
        "print(\"Instalação concluída.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbwxKypOSBkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442cd614-0f1f-44a6-bed3-f1f9379507ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas.\n"
          ]
        }
      ],
      "source": [
        "# @title Importar bibliotecas necessárias\n",
        "import os\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # Para suporte a vários modelos\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # Para criar conteúdo/partes da mensagem\n",
        "\n",
        "import warnings\n",
        "# Ignorar todos os avisos\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Bibliotecas importadas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mNsVI5eSDOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f8ac16-8d44-464c-c9b7-6d3c2ce447fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chaves de API Definidas:\n",
            "Chave da API Google definida: Sim\n",
            "Chave da API OpenAI definida: Não (SUBSTITUA O PLACEHOLDER!)\n",
            "Chave da API Anthropic definida: Não (SUBSTITUA O PLACEHOLDER!)\n"
          ]
        }
      ],
      "source": [
        "# @title Configure as Chaves de API (Substitua pelas suas chaves reais!)\n",
        "\n",
        "# --- IMPORTANTE: Substitua os placeholders pelas suas chaves de API reais ---\n",
        "from google.colab import userdata\n",
        "\n",
        "# Chave da API Gemini (Obtenha no Google AI Studio: https://aistudio.google.com/app/apikey)\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"SUA_CHAVE_DE_API_DO_GOOGLE\" # <--- SUBSTITUA\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# [Opcional]\n",
        "# Chave da API OpenAI (Obtenha na Plataforma OpenAI: https://platform.openai.com/api-keys)\n",
        "# os.environ['OPENAI_API_KEY'] = 'SUA_CHAVE_DE_API_DA_OPENAI' # <--- SUBSTITUA\n",
        "\n",
        "# [Opcional]\n",
        "# Chave da API Anthropic (Obtenha no Console da Anthropic: https://console.anthropic.com/settings/keys)\n",
        "# os.environ['ANTHROPIC_API_KEY'] = 'SUA_CHAVE_DE_API_DA_ANTHROPIC' # <--- SUBSTITUA\n",
        "\n",
        "\n",
        "# --- Verificar Chaves (Checagem Opcional) ---\n",
        "print(\"Chaves de API Definidas:\")\n",
        "print(f\"Chave da API Google definida: {'Sim' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'SUA_CHAVE_DE_API_DO_GOOGLE' else 'Não (SUBSTITUA O PLACEHOLDER!)'}\")\n",
        "print(f\"Chave da API OpenAI definida: {'Sim' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'SUA_CHAVE_DE_API_DA_OPENAI' else 'Não (SUBSTITUA O PLACEHOLDER!)'}\")\n",
        "print(f\"Chave da API Anthropic definida: {'Sim' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'SUA_CHAVE_DE_API_DA_ANTHROPIC' else 'Não (SUBSTITUA O PLACEHOLDER!)'}\")\n",
        "\n",
        "# Configure o ADK para usar as chaves de API diretamente (não o Vertex AI para esta configuração multi-modelo)\n",
        "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
        "\n",
        "\n",
        "# @markdown **Nota de Segurança:** A melhor prática é gerenciar as chaves de API de forma segura (por exemplo, usando Colab Secrets ou variáveis de ambiente) em vez de codificá-las diretamente no notebook. Substitua as strings de placeholder acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI_qvZJrSJuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b8b69b2-247d-4970-85ab-4a57b466eb9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ambiente configurado.\n"
          ]
        }
      ],
      "source": [
        "# --- Defina Constantes de Modelo para facilitar o uso ---\n",
        "\n",
        "# Mais modelos suportados podem ser referenciados aqui: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "# Mais modelos suportados podem ser referenciados aqui: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
        "MODEL_GPT_4O = \"openai/gpt-4.1\" # Você também pode tentar: gpt-4.1-mini, gpt-4o etc.\n",
        "\n",
        "# Mais modelos suportados podem ser referenciados aqui: https://docs.litellm.ai/docs/providers/anthropic\n",
        "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # Você também pode tentar: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
        "\n",
        "print(\"\\nAmbiente configurado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LZM3ysSOMu"
      },
      "source": [
        "Com certeza! Aqui está a tradução do texto para o português, mantendo a formatação original:\n",
        "\n",
        "---\n",
        "\n",
        "## Passo 1: Seu Primeiro Agente - Consulta Básica de Clima\n",
        "\n",
        "Vamos começar construindo o componente fundamental do nosso Bot de Clima: um único agente capaz de realizar uma tarefa específica – procurar informações sobre o clima. Isso envolve a criação de duas peças centrais:\n",
        "\n",
        "1.  **Uma Ferramenta:** Uma função Python que equipa o agente com a *habilidade* de buscar dados meteorológicos.\n",
        "2.  **Um Agente:** O \"cérebro\" de IA que entende a solicitação do usuário, sabe que possui uma ferramenta de clima e decide quando e como usá-la.\n",
        "\n",
        "---\n",
        "**1. Defina a Ferramenta (`get_weather`)**\n",
        "\n",
        "No ADK, as **Ferramentas** (Tools) são os blocos de construção que dão aos agentes capacidades concretas além da simples geração de texto. Elas são tipicamente funções Python comuns que realizam ações específicas, como chamar uma API, consultar um banco de dados ou fazer cálculos.\n",
        "\n",
        "Nossa primeira ferramenta fornecerá um relatório meteorológico *simulado* (mock). Isso nos permite focar na estrutura do agente sem a necessidade de chaves de API externas por enquanto. Mais tarde, você poderia facilmente trocar essa função simulada por uma que chame um serviço de clima real.\n",
        "**Conceito-Chave: Docstrings são Cruciais!** A LLM do agente depende muito da **docstring** da função para entender:\n",
        "\n",
        "*   *O que* a ferramenta faz.\n",
        "*   *Quando* usá-la.\n",
        "*   *Quais argumentos* ela requer (`city: str`).\n",
        "*   *Que informação* ela retorna.\n",
        "**Melhor Prática:** Escreva docstrings claras, descritivas e precisas para suas ferramentas. Isso é essencial para que a LLM use a ferramenta corretamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILy7YTCbSRAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fd144cb-c36e-4af6-de06-69d8558ac901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ferramenta: get_weather chamada para a cidade: Nova York ---\n",
            "{'status': 'success', 'report': 'O tempo em Nova York está ensolarado com uma temperatura de 25°C.'}\n",
            "--- Ferramenta: get_weather chamada para a cidade: Goiania ---\n",
            "{'status': 'success', 'report': 'Goiânia voltou a chover recentemente e está com uma agradável temperatura de 22°C.'}\n"
          ]
        }
      ],
      "source": [
        "# @title Defina a Ferramenta get_weather\n",
        "def get_weather(city: str) -> dict:\n",
        "  \"\"\"Recupera o boletim meteorológico atual para uma cidade especificada.\n",
        "\n",
        "  Args:\n",
        "      city (str): O nome da cidade (por exemplo, \"Nova York\", \"Londres\", \"Tóquio\").\n",
        "\n",
        "  Returns:\n",
        "      dict: Um dicionário contendo as informações meteorológicas.\n",
        "            Inclui uma chave 'status' ('success' ou 'error').\n",
        "            Se 'success', inclui uma chave 'report' com detalhes do tempo.\n",
        "            Se 'error', inclui uma chave 'error_message'.\n",
        "  \"\"\"\n",
        "  print(f\"--- Ferramenta: get_weather chamada para a cidade: {city} ---\") # Registra a execução da ferramenta\n",
        "  city_normalized = city.lower().replace(\" \", \"\") # Normalização básica\n",
        "\n",
        "  # Dados meteorológicos simulados (mock)\n",
        "  mock_weather_db = {\n",
        "      \"novayork\": {\"status\": \"success\", \"report\": \"O tempo em Nova York está ensolarado com uma temperatura de 25°C.\"},\n",
        "      \"londres\": {\"status\": \"success\", \"report\": \"Está nublado em Londres com uma temperatura de 15°C.\"},\n",
        "      \"tokyo\": {\"status\": \"success\", \"report\": \"Tóquio está com chuva fraca e uma temperatura de 18°C.\"},\n",
        "      \"goiania\": {\"status\": \"success\", \"report\": \"Goiânia voltou a chover recentemente e está com uma agradável temperatura de 22°C.\"},\n",
        "  }\n",
        "\n",
        "  if city_normalized in mock_weather_db:\n",
        "      return mock_weather_db[city_normalized]\n",
        "  else:\n",
        "      return {\"status\": \"error\", \"error_message\": f\"Desculpe, não tenho informações meteorológicas para '{city}'.\"}\n",
        "\n",
        "# Exemplo de uso da ferramenta (teste opcional)\n",
        "print(get_weather(\"Nova York\"))\n",
        "print(get_weather(\"Goiania\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAM0BqGWSTo5"
      },
      "source": [
        "**2. Defina o Agente (`weather_agent`)**\n",
        "\n",
        "Agora, vamos criar o **Agente** em si. Um `Agent` no ADK orquestra a interação entre o usuário, a LLM e as ferramentas disponíveis.\n",
        "\n",
        "Nós o configuramos com vários parâmetros-chave:\n",
        "\n",
        "*   `name`: Um identificador único para este agente (por exemplo, \"weather\\_agent\\_v1\").\n",
        "*   `model`: Especifica qual LLM usar (por exemplo, `MODEL_GEMINI_2_0_FLASH`). Começaremos com um modelo Gemini específico.\n",
        "*   `description`: Um resumo conciso do propósito geral do agente. Isso se torna crucial mais tarde, quando outros agentes precisarem decidir se devem delegar tarefas para *este* agente.\n",
        "*   `instruction`: Orientação detalhada para a LLM sobre como se comportar, sua persona, seus objetivos e, especificamente, *como e quando* utilizar suas `tools` (ferramentas) atribuídas.\n",
        "*   `tools`: Uma lista contendo as funções Python reais das ferramentas que o agente tem permissão para usar (por exemplo, `[get_weather]`).\n",
        "\n",
        "**Melhor Prática:** Forneça prompts de `instruction` (instrução) claros e específicos. Quanto mais detalhadas as instruções, melhor a LLM pode entender seu papel e como usar suas ferramentas de forma eficaz. Seja explícito sobre o tratamento de erros, se necessário.\n",
        "\n",
        "**Melhor Prática:** Escolha valores descritivos para `name` (nome) e `description` (descrição). Eles são usados internamente pelo ADK e são vitais para recursos como a delegação automática (abordada mais tarde)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ho1COmKSUeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936368db-f226-4ed2-8504-dbd0780bb450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agente 'weather_agent_v1' criado usando o modelo 'gemini-2.0-flash'.\n"
          ]
        }
      ],
      "source": [
        "# @title Defina o Agente de Clima\n",
        "# Use uma das constantes de modelo definidas anteriormente\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Começando com Gemini\n",
        "\n",
        "weather_agent = Agent(\n",
        "    name=\"weather_agent_v1\",\n",
        "    model=AGENT_MODEL, # Pode ser uma string para o Gemini ou um objeto LiteLlm\n",
        "    description=\"Fornece informações meteorológicas para cidades específicas.\",\n",
        "    instruction=\"Você é um assistente de clima prestativo. \"\n",
        "                \"Quando o usuário perguntar sobre o clima em uma cidade específica, \"\n",
        "                \"use a ferramenta 'get_weather' para encontrar a informação. \"\n",
        "                \"Se a ferramenta retornar um erro, informe o usuário educadamente. \"\n",
        "                \"Se a ferramenta for bem-sucedida, apresente o boletim meteorológico de forma clara.\",\n",
        "    tools=[get_weather], # Passe a função diretamente\n",
        ")\n",
        "\n",
        "print(f\"Agente '{weather_agent.name}' criado usando o modelo '{AGENT_MODEL}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvz7LDhbSZxL"
      },
      "source": [
        "\n",
        "---\n",
        "**3. Configurar o Runner e o Session Service**\n",
        "\n",
        "Para gerenciar as conversas e executar o agente, precisamos de mais dois componentes:\n",
        "\n",
        "*   `SessionService`: Responsável por gerenciar o histórico de conversas e o estado para diferentes usuários e sessões. O `InMemorySessionService` é uma implementação simples que armazena tudo na memória, adequada para testes e aplicações simples. Ele mantém o registro das mensagens trocadas. Exploraremos a persistência de estado mais a fundo no Passo 4.\n",
        "*   `Runner`: O motor que orquestra o fluxo de interação. Ele recebe a entrada do usuário, a direciona para o agente apropriado, gerencia as chamadas para a LLM e as ferramentas com base na lógica do agente, lida com as atualizações de sessão através do `SessionService` e gera eventos que representam o progresso da interação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h30dNtqMSah5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1faf68f-a090-4dce-f361-b24e3463fdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sessão criada: App='weather_tutorial_app', Usuário='user_1', Sessão='session_001'\n",
            "Runner criado para o agente 'weather_agent_v1'.\n"
          ]
        }
      ],
      "source": [
        "# @title Configurar o Session Service e o Runner\n",
        "\n",
        "# --- Gerenciamento de Sessão ---\n",
        "# Conceito-Chave: O SessionService armazena o histórico e o estado da conversa.\n",
        "# O InMemorySessionService é um armazenamento simples e não persistente para este tutorial.\n",
        "session_service = InMemorySessionService()\n",
        "\n",
        "# Defina constantes para identificar o contexto da interação\n",
        "APP_NAME = \"weather_tutorial_app\"\n",
        "USER_ID = \"user_1\"\n",
        "SESSION_ID = \"session_001\" # Usando um ID fixo para simplificar\n",
        "\n",
        "# Crie a sessão específica onde a conversa acontecerá\n",
        "session = await session_service.create_session(\n",
        "    app_name=APP_NAME,\n",
        "    user_id=USER_ID,\n",
        "    session_id=SESSION_ID\n",
        ")\n",
        "print(f\"Sessão criada: App='{APP_NAME}', Usuário='{USER_ID}', Sessão='{SESSION_ID}'\")\n",
        "\n",
        "# --- Runner ---\n",
        "# Conceito-Chave: O Runner orquestra o ciclo de execução do agente.\n",
        "runner = Runner(\n",
        "    agent=weather_agent, # O agente que queremos executar\n",
        "    app_name=APP_NAME,   # Associa as execuções ao nosso aplicativo\n",
        "    session_service=session_service # Usa nosso gerenciador de sessão\n",
        ")\n",
        "print(f\"Runner criado para o agente '{runner.agent.name}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zKGVwRkSduA"
      },
      "source": [
        "---\n",
        "**4. Interaja com o Agente**\n",
        "\n",
        "Precisamos de uma maneira de enviar mensagens ao nosso agente e receber suas respostas. Como as chamadas à LLM e a execução de ferramentas podem levar tempo, o `Runner` do ADK opera de forma assíncrona.\n",
        "\n",
        "Vamos definir uma função auxiliar `async` (`call_agent_async`) que:\n",
        "\n",
        "1.  Recebe uma string com a consulta do usuário.\n",
        "2.  A empacota no formato `Content` do ADK.\n",
        "3.  Chama `runner.run_async`, fornecendo o contexto de usuário/sessão e a nova mensagem.\n",
        "4.  Itera através dos **Eventos** (Events) gerados pelo runner. Os eventos representam os passos na execução do agente (por exemplo, chamada de ferramenta solicitada, resultado da ferramenta recebido, pensamento intermediário da LLM, resposta final).\n",
        "5.  Identifica e imprime o evento de **resposta final** usando `event.is_final_response()`.\n",
        "\n",
        "**Por que `async`?** As interações com LLMs e, potencialmente, com ferramentas (como APIs externas) são operações limitadas por I/O (entrada/saída). O uso de `asyncio` permite que o programa lide com essas operações de forma eficiente, sem bloquear a execução."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZJr8lbkSebH"
      },
      "outputs": [],
      "source": [
        "# @title Defina a Função de Interação com o Agente\n",
        "\n",
        "from google.genai import types # Para criar Conteúdo/Partes de mensagem\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Envia uma consulta ao agente e imprime a resposta final.\"\"\"\n",
        "  print(f\"\\n>>> Consulta do Usuário: {query}\")\n",
        "\n",
        "  # Prepare a mensagem do usuário no formato ADK\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "  final_response_text = \"O agente não produziu uma resposta final.\" # Padrão\n",
        "\n",
        "  # Conceito-Chave: run_async executa a lógica do agente e gera Eventos (Events).\n",
        "  # Iteramos através dos eventos para encontrar a resposta final.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # Você pode descomentar a linha abaixo para ver *todos* os eventos durante a execução\n",
        "      # print(f\"  [Evento] Autor: {event.author}, Tipo: {type(event).__name__}, Final: {event.is_final_response()}, Conteúdo: {event.content}\")\n",
        "\n",
        "      # Conceito-Chave: is_final_response() marca a mensagem de conclusão do turno.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "              # Assumindo uma resposta em texto na primeira parte\n",
        "              final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Lide com erros/escalonamentos potenciais\n",
        "              final_response_text = f\"Agente escalonou: {event.error_message or 'Nenhuma mensagem específica.'}\"\n",
        "          # Adicione mais verificações aqui se necessário (por exemplo, códigos de erro específicos)\n",
        "          break # Pare de processar eventos assim que a resposta final for encontrada\n",
        "\n",
        "  print(f\"<<< Resposta do Agente: {final_response_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6DQSqrqk5ic"
      },
      "source": [
        "---\n",
        "**5. Execute a Conversa**\n",
        "\n",
        "Finalmente, vamos testar nossa configuração enviando algumas consultas ao agente. Nós encapsulamos nossas chamadas `async` em uma função `async` principal e a executamos usando `await`.\n",
        "\n",
        "Observe a saída:\n",
        "\n",
        "*   Veja as consultas do usuário.\n",
        "*   Note os logs `--- Ferramenta: get_weather chamada... ---` quando o agente usa a ferramenta.\n",
        "*   Observe as respostas finais do agente, incluindo como ele lida com o caso em que os dados meteorológicos não estão disponíveis (para Paris)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEd2QhHyUKY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca400d8-f7f2-4688-9b11-83a8939bbb9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Consulta do Usuário: Como está o tempo em Londres?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ferramenta: get_weather chamada para a cidade: Londres ---\n",
            "<<< Resposta do Agente: Está nublado em Londres com uma temperatura de 15°C.\n",
            "\n",
            "\n",
            ">>> Consulta do Usuário: E em Paris?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ferramenta: get_weather chamada para a cidade: Paris ---\n",
            "<<< Resposta do Agente: Desculpe, não tenho informações meteorológicas para Paris.\n",
            "\n",
            ">>> Consulta do Usuário: Me diga o tempo em Nova York\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ferramenta: get_weather chamada para a cidade: Nova York ---\n",
            "<<< Resposta do Agente: O tempo em Nova York está ensolarado com uma temperatura de 25°C.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Execute a Conversa Inicial\n",
        "\n",
        "# Precisamos de uma função async para aguardar nosso auxiliar de interação\n",
        "async def run_conversation():\n",
        "    await call_agent_async(\"Como está o tempo em Londres?\",\n",
        "                                 runner=runner,\n",
        "                                 user_id=USER_ID,\n",
        "                                 session_id=SESSION_ID)\n",
        "\n",
        "    await call_agent_async(\"E em Paris?\",\n",
        "                                 runner=runner,\n",
        "                                 user_id=USER_ID,\n",
        "                                 session_id=SESSION_ID) # Esperando a mensagem de erro da ferramenta\n",
        "\n",
        "    await call_agent_async(\"Me diga o tempo em Nova York\",\n",
        "                                 runner=runner,\n",
        "                                 user_id=USER_ID,\n",
        "                                 session_id=SESSION_ID)\n",
        "\n",
        "# Execute a conversa usando await em um contexto assíncrono (como Colab/Jupyter)\n",
        "await run_conversation()\n",
        "\n",
        "# --- OU ---\n",
        "\n",
        "# Descomente as linhas a seguir se estiver executando como um script Python padrão (arquivo .py):\n",
        "# import asyncio\n",
        "# if __name__ == \"__main__\":\n",
        "#     try:\n",
        "#         asyncio.run(run_conversation())\n",
        "#     except Exception as e:\n",
        "#         print(f\"Ocorreu um erro: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbUzAGvsmB2a"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "Parabéns! Você construiu e interagiu com sucesso com seu primeiro agente ADK. Ele entende a solicitação do usuário, usa uma ferramenta para encontrar informações e responde adequadamente com base no resultado da ferramenta.\n",
        "\n",
        "No próximo passo, exploraremos como trocar facilmente o Modelo de Linguagem (Language Model) que alimenta este agente."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}